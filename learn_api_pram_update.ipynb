{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a39ba179",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "##Learn API PRAMUDYA R\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Import library for Flask\n",
    "from flask import Flask, jsonify, request\n",
    "from flasgger import Swagger, LazyString, LazyJSONEncoder\n",
    "from flasgger import swag_from\n",
    "\n",
    "# IMPORT ABUSIVE.CSV AND NEW_KAMUSALAY.CSV TO PANDAS DATAFRAME (EACH)\n",
    "df_abusive = pd.read_csv('abusive.csv')\n",
    "df_kamusalay = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "df_kamusalay.columns=[\"tidak baku\",\"baku\"]\n",
    "\n",
    "\n",
    "# Define Swagger UI description\n",
    "app = Flask(__name__)\n",
    "app.json_encoder = LazyJSONEncoder\n",
    "swagger_template = dict(\n",
    "info = {\n",
    "    'title': LazyString(lambda: 'API Documentation for Data Processing and Modeling'),\n",
    "    'version': LazyString(lambda: '1.0.0'),\n",
    "    'description': LazyString(lambda: 'Dokumentasi API untuk Data Processing dan Modeling'),\n",
    "    },\n",
    "    host = LazyString(lambda: request.host)\n",
    ")\n",
    "swagger_config = {\n",
    "    \"headers\": [],\n",
    "    \"specs\": [\n",
    "        {\n",
    "            \"endpoint\": 'docs',\n",
    "            \"route\": '/docs.json',\n",
    "        }\n",
    "    ],\n",
    "    \"static_url_path\": \"/flasgger_static\",\n",
    "    \"swagger_ui\": True,\n",
    "    \"specs_route\": \"/docs/\"\n",
    "}\n",
    "swagger = Swagger(app, template=swagger_template,             \n",
    "                  config=swagger_config)\n",
    "\n",
    "\n",
    "# Define endpoint for \"Basic Get\"\n",
    "@swag_from(\"D:/KERJA/BOOTCAMP/BINAR/NEW PROJECT/Docs/hello_world.yml\", methods=['GET'])\n",
    "@app.route('/', methods=['GET'])\n",
    "def hello_world():\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Say Hello\",\n",
    "        'data': \"Hello World\",\n",
    "    }\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "@swag_from(\"D:/KERJA/BOOTCAMP/BINAR/NEW PROJECT/Docs/text_processing.yml\", methods=['POST'])\n",
    "@app.route('/text-processing', methods=['POST'])\n",
    "def text_processing():\n",
    "    \n",
    "    text = request.form.get('text')\n",
    "    \n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Processed text\",\n",
    "        'data': re.sub(r'[^a-zA-Z0-9]', ' ', text)\n",
    "    }\n",
    "    \n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "@swag_from(\"D:/KERJA/BOOTCAMP/BINAR/NEW PROJECT/Docs/text_processing_file.yml\", methods=['POST'])\n",
    "@app.route('/text-processing-file', methods=['POST'])\n",
    "def text_processing_file():\n",
    "    global post_df\n",
    "\n",
    "    file = request.files.get('file')\n",
    "    \n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Processed file\",\n",
    "        'data': \"Successfully posted csv file\",\n",
    "    }\n",
    "    \n",
    "    post_df = pd.read_csv(file, encoding='latin-1', nrows=100)\n",
    "    \n",
    "    post_df = post_df[['Tweet']]\n",
    "    \n",
    "    # DROP DUPLICATED TWEETS\n",
    "    post_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # CREATE NEW NUMBER OF CHARACTERS (NO_CHAR) COLUMN THAT CONSISTS OF LENGTH OF TWEET CHARACTERS\n",
    "    post_df['no_char'] = post_df['Tweet'].apply(len)\n",
    "    \n",
    "    # CREATE NEW NUMBER OF WORDS (NO_WORDS) COLUMN THAT CONSISTS OF NUMBER OF WORDS OF EACH TWEET\n",
    "    post_df['no_words'] = post_df['Tweet'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # CREATE A FUNCTION TO CLEAN DATA FROM ANY NON ALPHA-NUMERIC (AND NON-SPACE) CHARACTERS, AND STRIP IT FROM LEADING/TRAILING SPACES\n",
    "    def tweet_cleansing(x):\n",
    "        tweet = x\n",
    "        cleaned_tweet = re.sub(r'[^a-zA-Z0-9 ]','',tweet).strip()\n",
    "        return cleaned_tweet\n",
    "    \n",
    "    # APPLY THE TWEET_CLEANSING FUNCTION ON TWEET COLUMN, AND CREATE A NEW CLEANED_TWEET COLUMN\n",
    "    post_df['cleaned_tweet'] = post_df['Tweet'].apply(lambda x: tweet_cleansing(x))\n",
    "    \n",
    "    # CREATE NEW NO_CHAR, AND NO_WORDS COLUMNS BASED ON CLEANED_TWEET COLUMN\n",
    "    post_df['no_char_2'] = post_df['cleaned_tweet'].apply(len)\n",
    "    post_df['no_words_2'] = post_df['cleaned_tweet'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # CREATE A FUNCTION TO COUNT NUMBER OF ABUSIVE WORDS FOUND IN A CLEANED TWEET\n",
    "    def count_abusive(x):\n",
    "        cleaned_tweet = x\n",
    "        matched_list = []\n",
    "        for i in range(len(df_abusive)):\n",
    "            for j in x.split():\n",
    "                word = df_abusive['ABUSIVE'].iloc[i]\n",
    "                if word==j.lower():\n",
    "                    matched_list.append(word)\n",
    "        return len(matched_list)\n",
    "    \n",
    "    # APPLY THE FUNCTION TO COUNT ABUSIVE WORDS, AND CREATE A NEW COLUMN BASED OFF OF IT\n",
    "    post_df['estimated_no_abs_words'] = post_df['cleaned_tweet'].apply(lambda x: count_abusive(x))\n",
    "    \n",
    "    # CONNECT / CREATE NEW DATABASE AND CREATE NEW TABLE CONSISTING LISTED TABLES\n",
    "    conn = sqlite3.connect('database_project.db')\n",
    "    q_create_table = \"\"\"\n",
    "    create table if not exists post_df (Tweet varchar(255), no_char int, no_words int, cleaned_tweet varchar(255), no_char_2 int, no_words_2 int);\n",
    "    \"\"\"\n",
    "    conn.execute(q_create_table)\n",
    "    conn.commit()\n",
    "    \n",
    "    # CHECK WHETHER TABLE ALREADY HAS DATA IN IT (TABLE HAS ROWS OF DATA IN IT)\n",
    "    cursor = conn.execute(\"select count(*) from post_df\")\n",
    "    num_rows = cursor.fetchall()\n",
    "    num_rows = num_rows[0][0]\n",
    "    \n",
    "    #  DO DATA INSERTIONS IF TABLE HAS NO DATA IN IT    \n",
    "    if num_rows == 0:\n",
    "    # DO ITERATIONS TO INSERT DATA (EACH ROW) FROM FINAL DATAFRAME (POST_DF)\n",
    "        for i in range(len(post_df)):\n",
    "            tweet = post_df['Tweet'].iloc[i]\n",
    "            no_char = int(post_df['no_char'].iloc[i])\n",
    "            no_words = int(post_df['no_words'].iloc[i])\n",
    "            cleaned_tweet = post_df['cleaned_tweet'].iloc[i]\n",
    "            no_char_2 = int(post_df['no_char_2'].iloc[i])\n",
    "            no_words_2 = int(post_df['no_words_2'].iloc[i])\n",
    "    \n",
    "            q_insertion = \"insert into post_df (Tweet, no_char, no_words, cleaned_tweet, no_char_2, no_words_2) values (?,?,?,?,?,?)\"\n",
    "            conn.execute(q_insertion,(tweet,no_char,no_words,cleaned_tweet,no_char_2,no_words_2))\n",
    "            conn.commit()    \n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    # VISUALIZE THE NUMBER OF ABUSIVE WORDS USING BARPLOT (COUNTPLOT)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    countplot = sns.countplot(data=post_df, x=\"estimated_no_abs_words\")\n",
    "    for p in countplot.patches:\n",
    "        countplot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()),  ha = 'center'\n",
    "                            , va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "    # %matplotlib inline\n",
    "    # warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "    plt.title('Count of Estimated Number of Abusive Words')\n",
    "    plt.xlabel('Estimated Number of Abusive Words')\n",
    "    plt.savefig('new_countplot.jpeg')\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    boxplot = sns.boxplot(data=post_df, x=\"no_words_2\")\n",
    "\n",
    "    print()\n",
    "    \n",
    "    # VISUALIZE THE NUMBER OF WORDS USING BOXPLOT\n",
    "    # %matplotlib inline\n",
    "    # warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "    plt.title('Number of Words Boxplot (after tweet cleansing)')\n",
    "    plt.xlabel('')\n",
    "    plt.savefig('new_boxplot.jpeg')\n",
    "    \n",
    "    # OUTPUT THE RESULT IN JSON FORMAT\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Teks yang sudah diproses\",\n",
    "        'data': list(post_df['cleaned_tweet'])\n",
    "    }\n",
    "    \n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135e00a8-9446-4a9b-8419-30066b73a3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_2",
   "language": "python",
   "name": "myenv_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
